# IND-WhoIsWho GNN思路

## 整体思路:节点分类

## 建图

我们对每个作者建立了一个图，也即 假设作者 DeepMayNotLearn 发表了500篇论文，我们会将这五百篇论文作为图中的节点，然后根据论文的信息进行交互，来添加边。

主要的四种边有：

1.共同作者，如果两篇论文中的共同作者超过1（也即是不包括作者本人），我们为两篇论文添加边

2.共同组织，如果两篇论文中的共同组织超过1（也即是不包括作者本人组织），我们为两篇论文添加边

3.共同关键词，如果两篇论文中的共同关键词超过0，我们为两篇论文添加边

4.共同期刊，如果两篇论文发表在同一期刊，我们为两篇论文添加边

对于上述的四种边，我们可以建立异质图，即添加四种类型的边，也可以建同质图，即不区分四种类型的边，在实际测试中，我们建立的图是第二种，同质图。缺点在于，建图速度特别慢，以训练集为例，775个作者大约要建7小时。也是因为一开始对于建图不够熟练，后续在充分学习后能够优化建图代码了。

一些尝试：

1.我们试过使用scibert基于两篇论文的语义相似度，或者oagbert的语义相似度，或w2v的语义相似度来构建图中的边，但是效果并不如人工构造的边，但是可以显著加快建图速度。

2.在边中最重要的边是共同作者，但同时共同作者也是最难被发现的边，要高效构造共同作者几乎是不可能的，因为在实际论文中，同一个作者名字的形式是差别巨大的，比如 Huang jianzhao,他的形式包括了jianzhao Huang,jianzao Huang,黄建朝，等多种名字，同时也有很多作者在一些论文中出现了缩写形式，因此在我们进行构建时，使用了pypinyin库将所有的中文名字转换为拼音，并且全部使用小写字符，且去除了其中的一些符号，通过前述的一些字符串，才能勉强提高一些识别的效果，但是要做到准确识别仍然是困难的，在我们的代码中，我们主要是排除了顺序的影响。但同时我们尝试过使用编辑距离进行匹配，即fuzzywuzzy库，能让效果得到很好提升，但是时间效率大大下降，因此取其轻，我们选择了较为简单的一种。其他的如组织、关键词、期刊的形式没有名字那么多样，所以较为容易。

## 模型

在搭建模型时，我们尝试了几乎所有的主流进行节点分类的图神经网络，如SAGE、GAT、SGC、ChebConv、GCNConv、GNNtransformers等，但是几乎任何一单一神经网络都没办法得到很好的结果，因此在我们的神经网络设计中，我们同时使用了SAGE、GAT、SGC、ChebConv进行图特征的提取，并且将他们提取的特征与初始特征进行拼接，任何送入mlp进行分类，但是这里我们在最后发现了一个点，如果我们在进行mlp时，不是直接预测而是将维度调整到80或者更少，再与四个图神经网络的特征拼接再送入分类层，大概给我们带来了两个点的提升，这一点，我们猜测是因为在分类前的mlp几乎进行了一个类似聚类的操作，这种聚类的操作能够让模型更能识别论文的类别。

## 一些后处理

我们尝试过一些后处理手段

1.标签传播算法：我们主要在测试集上使用标签传播算法，使用高置信度的节点，向其他节点进行标签传播，然后作为伪标签再次训练模型，不过效果是不太稳定的，在a榜带来了五个千分点的提升，但是在b榜几乎没有效果

2.CorrectAndSmooth(C&S):图神经网络上比较热门的一种半监督手段，在很多节点分类任务中得到了很好的效果，但是在本题中的效果同样有限，与标签传播算法差距不大。

## 更多的尝试

在此，我想讲一下对本赛题的理解并且讲述我根据badcase分析所得的结果。

本赛题旨在对一个重名作者进行消岐，其中标签有0有1，但是何种作者被标签为1，又或者何种作者被标签为0是不清晰的，个人的理解是，一个名字中假设包含了五个张三，其中拥有论文最多的那个张三应该被预测为1，其他的4个张三被标识为0，当然这是我站在测试集的角度进行考虑，因为在测试集中，我们并没有一个标签，如果是少样本，这道题还可以被认为是匹配型赛题。因为在我的验证集的badcase中，我发现很多作者的auc大概是0.3或者更低，也即是，将01预测反了，使用在本赛题中，发现真正例是很重要的，对模型较为有指导性的，这也能一定程度解释，我们模型在聚类后分类有一定提升的原因。

1.元学习（MAML）:在赛题初期，我们尝试了元学习，因为本赛题是不同于以往的节点分类赛题，正常的节点分类往往是单图且半监督的，而在多图，且每个图的构造不一，我们认为图神经网络模型很难进行学习，因此我们使用元学习，在多张图上进行学习，以期模型可以泛化到没有见过的图，但是效果惨淡

## 文件树

├── IND-WhoIsWho/
    ├── ind_test_author_filter_public.json   --B榜测试作者
    ├── ind_test_author_submit.json  --B榜提交示例
    ├── ind_valid_author.json    --A榜测试作者
    ├── ind_valid_author_submit.json    --A榜提交示例
    ├── pid_to_info_all.json    --全部的论文数据
    ├── stopwords_English.txt    --英文停用词，用于Word2Vec
    ├── train_author.json  --训练集作者
├── model/    --存储训练好的模型
├── result/     --存储预测结果
├── sci_graph/    --使用scibert作为特征建立的图
├── scibert_model/    --scibert模型
    ├── config.json
    ├── pytorch_model.bin
├── scibert_tokenizer/    --scibert分词器
    ├── vocab.txt
├── w2v_graph/    --使用w2v作为特征建立的图
├── build_graph.sh    --建立所有图的指令集
├── build_sci_graph_test.py    --使用scibert作为作为特征建立测试集图集合
├── build_sci_graph_train.py    --使用scibert作为作为特征建立训练集图集合
├── build_w2v_graph_test.py    --使用w2v作为作为特征建立测试集图集合
├── build_w2v_graph_train.py    --使用w2v作为作为特征建立训练集图集合
├── install_requirements.sh    --安装依赖的指令集，主要是安装dgl-cu121需要单独安装
├── model.py    --模型定义文件
├── requirements.txt    --环境依赖文件
├── sci_model_test.py   --scibert作为特征训练的图模型进行推理
├── train_model.sh    --训练两个图模型的指令集
├── train_sci_model.py    --训练scibert作为特征建立的图模型
├── train_w2v_model.py    --训练w2v作为特征建立的图模型
├── w2v_model_test.py    --w2v作为特征训练的图模型进行推理
├── word2vec_model.py    --训练Word2vec模型并将word2vec特征保存到论文中

